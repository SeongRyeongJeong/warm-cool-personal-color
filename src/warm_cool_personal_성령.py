# -*- coding: utf-8 -*-
"""warm_cool_personal_ì„±ë ¹

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sVMkASz5StPFWndvUDDU1Us_cxGNX4qv
"""

pip install -q opencv-python==4.10.0.84 scikit-image==0.24.0

from google.colab import drive
drive.mount('/content/drive')

# 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
from glob import glob
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import matplotlib.patches as patches
import dlib
from tensorflow.keras.models import load_model # ëª¨ë¸ ë¡œë“œì— í•„ìˆ˜

!pip install mediapipe gradio dlib opencv-python numpy matplotlib
# dlib shape predictorê°€ ì—†ë‹¤ë©´ ë‹¤ìš´ë¡œë“œ (ì´ë¯¸ ìˆë‹¤ë©´ ì£¼ì„ ì²˜ë¦¬)
!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2
!bzip2 -d shape_predictor_68_face_landmarks.dat.bz2

# ìƒìˆ˜ ì„¤ì •
IMAGE_SIZE = 224
BATCH_SIZE = 16
DATA_DIR = '/content/drive/MyDrive/ì˜ìƒì²˜ë¦¬/data/'
MODEL_SAVE_PATH = '/content/drive/MyDrive/ì˜ìƒì²˜ë¦¬/personal_color_mobilenetv2_model.h5'
TEST_IMAGE_PATH = '/content/drive/MyDrive/ì˜ìƒì²˜ë¦¬/data/t2.jpg'
PREDICTOR_PATH = "shape_predictor_68_face_landmarks.dat"

# 3. ë°ì´í„° ë¡œë“œ ë° ë¶„í•  (Deep Learning ì „ì²˜ë¦¬)

# ImageDataGeneratorë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì¦ê°• ë° í‘œì¤€í™”
train_datagen = ImageDataGenerator(
    rescale=1./255, # í”½ì…€ ê°’ ì •ê·œí™” (0~1)
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    validation_split=0.2  # í›ˆë ¨ ë°ì´í„°ì˜ 20%ë¥¼ ê²€ì¦ì— ì‚¬ìš©
)

# í•™ìŠµ ë°ì´í„° ìƒì„±ê¸°
# DATA_DIR ë‚´ë¶€ì˜ 'warm'ê³¼ 'cool' í´ë”ë¥¼ ìë™ìœ¼ë¡œ í´ë˜ìŠ¤ë¡œ ì¸ì‹í•©ë‹ˆë‹¤.
try:
    train_generator = train_datagen.flow_from_directory(
        DATA_DIR,
        target_size=(IMAGE_SIZE, IMAGE_SIZE),
        batch_size=BATCH_SIZE,
        class_mode='categorical', # ì›œí†¤, ì¿¨í†¤ 2ê°€ì§€ í´ë˜ìŠ¤
        subset='training'
    )

    # ê²€ì¦ ë°ì´í„° ìƒì„±ê¸°
    validation_generator = train_datagen.flow_from_directory(
        DATA_DIR,
        target_size=(IMAGE_SIZE, IMAGE_SIZE),
        batch_size=BATCH_SIZE,
        class_mode='categorical',
        subset='validation'
    )

    # í´ë˜ìŠ¤ ì´ë¦„ í™•ì¸
    class_names = list(train_generator.class_indices.keys())
    print(f"í´ë˜ìŠ¤ ì¸ë±ìŠ¤: {train_generator.class_indices}")

except Exception as e:
    print(f"ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    print("ê²½ë¡œì™€ í´ë” ì´ë¦„(warm, cool)ì„ ë‹¤ì‹œ í•œë²ˆ í™•ì¸í•´ì£¼ì„¸ìš”.")

# ----------------------------------------
# 4. ë”¥ëŸ¬ë‹ ëª¨ë¸ ì •ì˜ (ì „ì´ í•™ìŠµ - MobileNetV2)
# ----------------------------------------

# ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸ í›„ ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ ì§„í–‰
if train_generator.samples > 0:
    base_model = MobileNetV2(
        weights='imagenet',
        include_top=False,
        input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)
    )

    base_model.trainable = False # ë² ì´ìŠ¤ ëª¨ë¸ ê°€ì¤‘ì¹˜ ê³ ì •

    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(512, activation='relu')(x)
    x = Dropout(0.5)(x)
    predictions = Dense(len(class_names), activation='softmax')(x) # ìµœì¢… ì¶œë ¥: 2ê°œ í´ë˜ìŠ¤

    model = Model(inputs=base_model.input, outputs=predictions)

    model.compile(
        optimizer=Adam(learning_rate=0.0001),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    model.summary()

    # ----------------------------------------
    # 5. ëª¨ë¸ í•™ìŠµ
    # ----------------------------------------
    epochs = 20 # í…ŒìŠ¤íŠ¸ ëª©ì ì´ë¼ë©´ 5~10 ì •ë„ë¡œ ë‚®ì¶°ì„œ ë¹ ë¥´ê²Œ í™•ì¸ ê°€ëŠ¥

    history = model.fit(
        train_generator,
        steps_per_epoch=train_generator.samples // BATCH_SIZE,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // BATCH_SIZE,
        epochs=epochs
    )

    # ëª¨ë¸ ì €ì¥
    model.save('/content/drive/MyDrive/ì˜ìƒì²˜ë¦¬/personal_color_mobilenetv2_model.h5')

    # ----------------------------------------
    # 6. ì˜ˆì¸¡ ë° ì˜ìƒ ì²˜ë¦¬ ê¸°ë°˜ ì¶”ì²œ (ì¶”ê°€ ê¸°ëŠ¥)
    # ----------------------------------------

    # ... (ì˜ˆì¸¡ ë° ì¶”ì²œ í•¨ìˆ˜ëŠ” ì´ì „ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ ì ìš© ê°€ëŠ¥)
else:
    print("í•™ìŠµí•  ì´ë¯¸ì§€ê°€ 0ê°œì…ë‹ˆë‹¤. ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

# 1. ëª¨ë¸ ë¡œë“œ
try:
    if 'model' not in locals() or model is None:
        model = load_model(MODEL_SAVE_PATH)
        if 'class_names' not in locals():
            class_names = ['cool', 'warm']
        print("ëª¨ë¸ ë¡œë“œ ì„±ê³µ.")
except Exception as e:
    print(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. í•™ìŠµì´ ì™„ë£Œë˜ì—ˆê±°ë‚˜ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”: {e}")
    model = None

# 2. í”„ë ˆì„ ì „ì²˜ë¦¬ ë° ì˜ˆì¸¡ í•¨ìˆ˜
def preprocess_and_predict(frame, model, class_labels, image_size=IMAGE_SIZE):
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    img_input = cv2.resize(rgb_frame, (image_size, image_size))
    img_input = np.expand_dims(img_input / 255.0, axis=0)
    prediction = model.predict(img_input, verbose=0)[0]
    predicted_index = np.argmax(prediction)
    confidence = prediction[predicted_index] * 100
    predicted_label = class_labels[predicted_index]
    return predicted_label, confidence

# 3. ë¹„ë””ì˜¤ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
def process_video(video_path, output_path, model, class_names):
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"ì˜¤ë¥˜: ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”: {video_path}")
        return
    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))
    frame_count = 0
    predicted_color = "ë¶„ì„ ì¤‘..."
    conf = 0.0
    print(f"--- ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹œì‘: {os.path.basename(video_path)} ---")
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        # 5í”„ë ˆì„ë§ˆë‹¤ ì˜ˆì¸¡ ìˆ˜í–‰
        if frame_count % 5 == 0:
            predicted_color, conf = preprocess_and_predict(frame, model, class_names)
        text = f"Tone: {predicted_color} ({conf:.1f}%)"
        color_code = (0, 165, 255) if predicted_color.lower().find('warm') != -1 else (255, 0, 0)
        cv2.putText(frame, text, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, color_code, 3)
        out.write(frame)
        frame_count += 1
    cap.release()
    out.release()
    print(f"ë¹„ë””ì˜¤ ì²˜ë¦¬ ì™„ë£Œ. ê²°ê³¼ íŒŒì¼ ì €ì¥: {output_path}")

# ----------------------------------------
# 7.1.1 ìƒ‰ì¡° ì¶”ì²œ ë°ì´í„°ë² ì´ìŠ¤ ì •ì˜ (Hex Code ì‚¬ìš©)
# ----------------------------------------

# ì¿¨í†¤ ìƒ‰ìƒ íŒ”ë ˆíŠ¸ ë°ì´í„° (ì‚¬ìš©ìë‹˜ì˜ ì²¨ë¶€ ì´ë¯¸ì§€ ê¸°ë°˜)
COOL_TONE_PALETTE = {
    "Lip": ["#A3366C", "#C23B6B", "#E04F7A"],
    "Blush": ["#C57BA8", "#F2A4C0", "#D689B7"],
    "Base": ["#D1C7BC", "#DDD0C5", "#E8D8CC"],
    "Tone_Description": "ì—¬ë¦„/ê²¨ìš¸ ì¿¨í†¤ (Summer/Winter Cool)"
}

# ì›œí†¤ ìƒ‰ìƒ íŒ”ë ˆíŠ¸ ë°ì´í„° (ì˜ˆì‹œ)
WARM_TONE_PALETTE = {
    "Lip": ["#FFB084", "#E07A5F", "#B5653A"],
    "Blush": ["#FFC899", "#E0AA82", "#D49C8D"],
    "Base": ["#E8D3C8", "#D9C9B4", "#C6B4A8"],
    "Tone_Description": "ë´„/ê°€ì„ ì›œí†¤ (Spring/Autumn Warm)"
}

def get_recommendation_data(predicted_tone):
    """ì˜ˆì¸¡ëœ í†¤ì— ë”°ë¥¸ ìƒ‰ìƒ ë°ì´í„°ì™€ ì„¤ëª…ì„ ë°˜í™˜"""
    tone = predicted_tone.lower()

    if 'cool' in tone:
        return COOL_TONE_PALETTE
    elif 'warm' in tone:
        return WARM_TONE_PALETTE
    else:
        return None

# ----------------------------------------
# 7.1.2 ìƒ‰ìƒ íŒ”ë ˆíŠ¸ ì‹œê°í™” í•¨ìˆ˜
# ----------------------------------------

def visualize_palette(palette_data, image_path, predicted_label):
    """
    Matplotlibì„ ì‚¬ìš©í•˜ì—¬ ì¶”ì²œ ìƒ‰ìƒì„ íŒ”ë ˆíŠ¸ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.
    """
    if palette_data is None:
        print("ì‹œê°í™”í•  íŒ”ë ˆíŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì´ë¯¸ì§€ ë¡œë“œ (ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ê°€ ì•„ë‹Œ ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ë‹¤ì‹œ ë¡œë“œ)
    try:
        img = cv2.imread(image_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    except:
        print("ê²½ê³ : ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ì–´ ì´ë¯¸ì§€ ì—†ì´ íŒ”ë ˆíŠ¸ë§Œ í‘œì‹œí•©ë‹ˆë‹¤.")
        img = None

    fig = plt.figure(figsize=(10, 10))

    # 1. ìƒì–¼ ì‚¬ì§„ í‘œì‹œ (ì™¼ìª½)
    if img is not None:
        ax1 = fig.add_subplot(2, 2, 1)
        ax1.imshow(img)
        ax1.set_title(f"Original Photo: {os.path.basename(image_path)}", fontsize=14)
        ax1.axis('off')

        # í…ìŠ¤íŠ¸ ì§„ë‹¨ ê²°ê³¼ í‘œì‹œ
        ax1.text(0.5, -0.1,
                 f"Diagnosis: {predicted_label.upper()} ({palette_data['Tone_Description']})",
                 transform=ax1.transAxes,
                 fontsize=14,
                 weight='bold',
                 ha='center')
    else:
        # ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìœ¼ë©´, íŒ”ë ˆíŠ¸ê°€ ë” í° ê³µê°„ì„ ì°¨ì§€í•˜ë„ë¡ ì¡°ì • ê°€ëŠ¥
        pass

    # 2. ìƒ‰ìƒ íŒ”ë ˆíŠ¸ í‘œì‹œ (ì˜¤ë¥¸ìª½ ë° í•˜ë‹¨)
    palette_items = list(palette_data.keys())[:-1] # ë§ˆì§€ë§‰ 'Tone_Description' ì œì™¸

    for i, item in enumerate(palette_items):
        # ì¿¨í†¤ ì´ë¯¸ì§€ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„œë¸Œí”Œë¡¯ ìœ„ì¹˜ ì„¤ì • (ì‚¬ì§„ ìœ ë¬´ì— ë”°ë¼ ìœ„ì¹˜ ì¡°ì •)
        ax_pos = i + 2 if img is not None else i + 1
        ax = fig.add_subplot(len(palette_items), 2 if img is not None else 1, ax_pos)

        colors = palette_data[item]
        num_colors = len(colors)

        # ì‹¤ì œ ìƒ‰ìƒ ë¸”ë¡ ê·¸ë¦¬ê¸°
        for j, hex_color in enumerate(colors):
            # Hex to RGB (Matplotlibì€ Hex ì½”ë“œë¥¼ ì§ì ‘ ì§€ì›)
            rect = patches.Rectangle((j, 0), 1, 1, facecolor=hex_color, edgecolor='white', linewidth=1)
            ax.add_patch(rect)

            # Hex ì½”ë“œ í…ìŠ¤íŠ¸ í‘œì‹œ
            ax.text(j + 0.5, 0.5, hex_color, ha='center', va='center', fontsize=10, color='black' if sum(int(hex_color[k:k+2], 16) for k in (1, 3, 5)) > 300 else 'white')

        ax.set_xlim(0, num_colors)
        ax.set_ylim(0, 1)
        ax.set_title(f"{item} ({predicted_label.lower()})", fontsize=12)
        ax.axis('off')

    plt.tight_layout(rect=[0, 0, 1, 0.95]) # ì œëª©ê³¼ ê²¹ì¹˜ì§€ ì•Šë„ë¡ ì¡°ì •
    plt.suptitle(f"Personal Color Makeup Recommendation for {predicted_label.upper()}", fontsize=16, weight='bold')
    plt.show()

# ----------------------------------------
# 7.2 ë° 7.3 ì‹¤í–‰ ë¶€ë¶„ ìˆ˜ì •
# ----------------------------------------
# ê¸°ì¡´ 'test_single_image' í•¨ìˆ˜ ë‚´ë¶€ì˜ 4ë‹¨ê³„(ìƒ‰ì¡° ì¶”ì²œ)ë¥¼ ì•„ë˜ì™€ ê°™ì´ ìˆ˜ì •í•©ë‹ˆë‹¤.

def test_single_image(image_path, model, class_names, image_size=IMAGE_SIZE):
    if not os.path.exists(image_path):
        print(f"ì˜¤ë¥˜: í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”: {image_path}")
        return

    # 1. ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬ (ì´ì „ ì½”ë“œì™€ ë™ì¼)
    frame = cv2.imread(image_path)
    if frame is None:
        print("ì˜¤ë¥˜: ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    img_input = cv2.resize(rgb_frame, (image_size, image_size))
    img_input = np.expand_dims(img_input / 255.0, axis=0)

    # 2. ëª¨ë¸ ì˜ˆì¸¡ (ì´ì „ ì½”ë“œì™€ ë™ì¼)
    prediction = model.predict(img_input, verbose=0)[0]
    predicted_index = np.argmax(prediction)
    confidence = prediction[predicted_index] * 100
    predicted_label = class_names[predicted_index]

    # 3. ê²°ê³¼ ì¶œë ¥ (í…ìŠ¤íŠ¸)
    print("\n" + "="*50)
    print(f"âœ¨ ìƒì–¼ ì‚¬ì§„ í¼ìŠ¤ë„ ì»¬ëŸ¬ ì§„ë‹¨ ê²°ê³¼ ({os.path.basename(image_path)}) âœ¨")
    print("="*50)
    print(f"âœ… ì˜ˆì¸¡ëœ í†¤: {predicted_label.upper()} ({confidence:.2f}% í™•ì‹ )")

    # 4. ìƒ‰ì¡° ì¶”ì²œ ë° ì‹œê°í™” (ìˆ˜ì •ëœ ë¶€ë¶„)
    recommendations = get_recommendation_data(predicted_label)

    if recommendations:
        print(f"\n[ ì¶”ì²œ ìƒ‰ì¡° ê°€ì´ë“œ - {recommendations['Tone_Description']} ]")
        # í…ìŠ¤íŠ¸ë¡œ Hex ì½”ë“œ ì¶œë ¥
        for item, hex_list in recommendations.items():
            if item != 'Tone_Description':
                print(f"- {item}: {' '.join(hex_list)}")

        # 5. ì‹œê°í™” ì‹¤í–‰
        visualize_palette(recommendations, image_path, predicted_label)
    else:
        print("ì•Œ ìˆ˜ ì—†ëŠ” í†¤ì´ ì˜ˆì¸¡ë˜ì–´ ì¶”ì²œì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    print("="*50)

# ----------------------------------------
# 7.3 ì‹¤í–‰ (ë³€ë™ ì—†ìŒ)
# ----------------------------------------
# ëª¨ë¸ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ê²½ë¡œ ì„¤ì •ì€ ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€ë©ë‹ˆë‹¤.
# (MODEL_SAVE_PATH ë° TEST_IMAGE_PATH í™•ì¸ í•„ìˆ˜)

try:
    if 'model' not in locals() or model is None:
        model = load_model(MODEL_SAVE_PATH)
        print("ìµœì¢… ëª¨ë¸ ë¡œë“œ ì„±ê³µ.")

    # t1 ì‚¬ì§„ìœ¼ë¡œ ìµœì¢… ì§„ë‹¨ ë° ì‹œê°í™” ì‹¤í–‰
    test_single_image(TEST_IMAGE_PATH, model, class_names)

except Exception as e:
    print(f"ì˜ˆì¸¡ ë‹¨ê³„ ì˜¤ë¥˜: {e}")
    print("ëª¨ë¸ íŒŒì¼ ê²½ë¡œ(`MODEL_SAVE_PATH`) ë˜ëŠ” í´ë˜ìŠ¤ ì´ë¦„(`class_names`) ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")

# Dlib ì„¤ì¹˜ (ì»´íŒŒì¼ ì‹œê°„ì´ ë‹¤ì†Œ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)
!pip install dlib opencv-python numpy

# ëœë“œë§ˆí¬ ì˜ˆì¸¡ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
# ì´ íŒŒì¼ì€ ì–¼êµ´ì˜ 68ê°œ íŠ¹ì§•ì (ì…ìˆ , ëˆˆ ë“±)ì„ ì°¾ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2
!bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2

import dlib
import cv2
import numpy as np
import gradio as gr
import os

# ==========================================
# 1. Dlib ëœë“œë§ˆí¬ ëª¨ë¸ ì„¤ì • (ê°€ì¥ ì¤‘ìš”!)
# ==========================================
PREDICTOR_PATH = "shape_predictor_68_face_landmarks.dat"

# íŒŒì¼ ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ
if not os.path.exists(PREDICTOR_PATH):
    print("ğŸ“¥ ëœë“œë§ˆí¬ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
    os.system("wget -O shape_predictor_68_face_landmarks.dat https://github.com/italojs/facial-landmarks-recognition/raw/master/shape_predictor_68_face_landmarks.dat")

# ì—¬ê¸°ì„œ detectorì™€ predictorë¥¼ 'ì „ì—­ ë³€ìˆ˜'ë¡œ í™•ì‹¤í•˜ê²Œ ì„ ì–¸
detector = dlib.get_frontal_face_detector()
try:
    predictor = dlib.shape_predictor(PREDICTOR_PATH)
    print("âœ… Predictor ë¡œë“œ ì„±ê³µ!")
except Exception as e:
    print(f"ğŸš¨ Predictor ë¡œë“œ ì‹¤íŒ¨: {e}")

# ==========================================
# 2. ë©”ì´í¬ì—… & ë¶„ì„ í•¨ìˆ˜ ì •ì˜
# ==========================================
def apply_lipstick_virtual(frame, landmarks, color_bgr, alpha=0.4):
    points = landmarks[48:68]
    hull = cv2.convexHull(points)
    mask = np.zeros(frame.shape[:2], dtype=np.uint8)
    cv2.drawContours(mask, [hull], 0, 255, -1)
    mask = cv2.GaussianBlur(mask, (15, 15), 0)

    color_layer = np.zeros_like(frame, dtype=np.uint8)
    color_layer[:] = color_bgr

    frame_float = frame.astype(float)
    color_layer_float = color_layer.astype(float)
    mask_float = mask.astype(float) / 255.0
    mask_3ch = cv2.merge([mask_float, mask_float, mask_float])

    output = frame_float * (1.0 - alpha * mask_3ch) + color_layer_float * (alpha * mask_3ch)
    return output.astype(np.uint8)

def apply_eyebrow_tint(frame, landmarks, color_bgr, alpha=0.3):
    result = frame.astype(float)
    color_layer = np.zeros_like(frame, dtype=np.uint8)
    color_layer[:] = color_bgr
    color_layer = color_layer.astype(float)
    mask_total = np.zeros(frame.shape[:2], dtype=np.float32)

    for idx_range in [(17, 22), (22, 27)]:
        points = landmarks[idx_range[0]:idx_range[1]]
        hull = cv2.convexHull(points)
        mask = np.zeros(frame.shape[:2], dtype=np.uint8)
        cv2.drawContours(mask, [hull], 0, 255, -1)
        mask = cv2.GaussianBlur(mask, (11, 11), 0)
        mask_float = mask.astype(float) / 255.0
        mask_total = np.maximum(mask_total, mask_float)

    mask_3ch = cv2.merge([mask_total, mask_total, mask_total])
    output = result * (1.0 - alpha * mask_3ch) + color_layer * (alpha * mask_3ch)
    return output.astype(np.uint8)

def analyze_skin_tone(image, landmarks):
    pt1 = landmarks[2]; pt2 = landmarks[31]
    x_min, x_max = min(pt1[0], pt2[0]), max(pt1[0], pt2[0])
    y_min, y_max = min(pt1[1], pt2[1]), max(pt1[1], pt2[1])
    cheek_roi = image[y_min:y_max, x_min:x_max]

    if cheek_roi.size == 0: return "ë¶„ì„ ì‹¤íŒ¨", 0, 0, 0

    lab_roi = cv2.cvtColor(cheek_roi, cv2.COLOR_BGR2LAB)
    l_mean = np.mean(lab_roi[:,:,0])
    a_mean = np.mean(lab_roi[:,:,1])
    b_mean = np.mean(lab_roi[:,:,2])

    return f"â€¢ ë°ê¸°(L): {l_mean:.1f}\nâ€¢ ë¶‰ì€ê¸°(a): {a_mean-128:.1f}\nâ€¢ ë…¸ë€ê¸°(b): {b_mean-128:.1f}"

# ==========================================
# 3. ë°ì´í„° ë° ì„¤ì •
# ==========================================
MAKEUP_PALETTES = {
    "cool": {"lip": (147, 112, 219), "eyebrow": (60, 60, 60)}, # BGR
    "warm": {"lip": (80, 90, 255), "eyebrow": (40, 70, 100)}
}

PRODUCT_DB = {
    "cool": [
        {"brand": "ë¡¬ì•¤", "name": "ì¥¬ì‹œ ë˜ìŠ¤íŒ… í‹´íŠ¸ #ë² ì–´ê·¸ë ˆì´í”„", "desc": "ì°¨ë¶„í•œ ì¿¨í†¤ í•‘í¬"},
        {"brand": "í˜ë¦¬í˜ë¼", "name": "ì‰í¬ ë¬´ë“œ ê¸€ë¡œì´ #ê°“ê¸°ì²œì‚¬", "desc": "ì—¬ì¿¨ë¼ ì¶”ì²œ"}
    ],
    "warm": [
        {"brand": "í—¤ë¼", "name": "ì„¼ìŠˆì–¼ íŒŒìš°ë” ë§¤íŠ¸ #íŒœíŒŒìŠ¤", "desc": "ì›œí†¤ êµ­ë¯¼í…œ"},
        {"brand": "3CE", "name": "ë²¨ë²³ ë¦½ í‹´íŠ¸ #ë‹¤í¬ë”œ", "desc": "ê°€ì„ ì›œí†¤ ì¶”ì²œ"}
    ]
}

# ==========================================
# 4. Gradio ë©”ì¸ ë¡œì§
# ==========================================
def process_oliveyoung_style(input_image):
    if input_image is None: return None, "ì‚¬ì§„ì„ ë„£ì–´ì£¼ì„¸ìš”", ""

    # 1. ëª¨ë¸ í™•ì¸ (model ë³€ìˆ˜ê°€ ë©”ëª¨ë¦¬ì— ìˆëŠ”ì§€ ì²´í¬)
    if 'model' not in globals():
        return input_image, "ğŸš¨ ì˜¤ë¥˜: AI ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìœ„ì—ì„œ model=load_model(...) ì½”ë“œë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.", ""

    frame_bgr = cv2.cvtColor(input_image, cv2.COLOR_RGB2BGR)
    img_resized = cv2.resize(frame_bgr, (224, 224)) # IMAGE_SIZE ëŒ€ì‹  224 ì§ì ‘ ì…ë ¥
    img_input_norm = np.expand_dims(img_resized / 255.0, axis=0)

    # 2. ì˜ˆì¸¡
    prediction = model.predict(img_input_norm, verbose=0)[0]
    idx = np.argmax(prediction)
    label = class_names[idx] if 'class_names' in globals() else ('cool' if idx==0 else 'warm')

    # 3. ë©”ì´í¬ì—… & ë¶„ì„
    faces = detector(frame_bgr, 1)
    analysis = "ì–¼êµ´ ê°ì§€ ì‹¤íŒ¨"

    if faces:
        lm = predictor(frame_bgr, faces[0]) # ì—¬ê¸°ì„œ ì˜¤ë¥˜ê°€ ë‚¬ì—ˆìŒ (ì´ì œ í•´ê²°ë¨)
        pts = np.array([[p.x, p.y] for p in lm.parts()])

        analysis = analyze_skin_tone(frame_bgr, pts)

        tone_key = 'cool' if 'cool' in label else 'warm'
        palette = MAKEUP_PALETTES[tone_key]

        frame_bgr = apply_eyebrow_tint(frame_bgr, pts, palette["eyebrow"])
        frame_bgr = apply_lipstick_virtual(frame_bgr, pts, palette["lip"], alpha=0.5)

    output_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)

    # 4. ê²°ê³¼ í…ìŠ¤íŠ¸
    rec_text = ""
    recs = PRODUCT_DB.get('cool' if 'cool' in label else 'warm', [])
    for r in recs: rec_text += f"[{r['brand']}] {r['name']}\n"

    return output_rgb, f"ë‹¹ì‹ ì€ {label.upper()}í†¤ ì…ë‹ˆë‹¤.\n{analysis}", rec_text

# ==========================================
# 5. ì•± ì‹¤í–‰
# ==========================================
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("## ğŸ’„ AI í¼ìŠ¤ë„ ì»¬ëŸ¬ ì§„ë‹¨ (Olive Young Ver.)")

    # -------------------------
    # 1) ì—…ë¡œë“œìš© (ì •ì  ì´ë¯¸ì§€)
    # -------------------------
    gr.Markdown("### ğŸ–¼ ì´ë¯¸ì§€ ì—…ë¡œë“œ ì§„ë‹¨")
    with gr.Row():
        inp_upload = gr.Image(
            label="ì–¼êµ´ ì‚¬ì§„ ì—…ë¡œë“œ",
            type="numpy",
            sources=["upload"],   # ì—…ë¡œë“œë§Œ
            streaming=False
        )
        out_upload = gr.Image(label="ë©”ì´í¬ì—… ê²°ê³¼ (ì—…ë¡œë“œ)")
    with gr.Row():
        txt_res_upload = gr.Textbox(label="ë¶„ì„ ê²°ê³¼ (ì—…ë¡œë“œ)")
        txt_rec_upload = gr.Textbox(label="ì¶”ì²œ ì œí’ˆ (ì—…ë¡œë“œ)")

    inp_upload.change(
        fn=process_oliveyoung_style,
        inputs=inp_upload,
        outputs=[out_upload, txt_res_upload, txt_rec_upload]
    )

    # -------------------------
    # 2) ì›¹ìº ìš© (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°)
    # -------------------------
    gr.Markdown("### ğŸ¥ ì‹¤ì‹œê°„ ì›¹ìº  ì§„ë‹¨")
    with gr.Row():
        cam = gr.Image(
            label="ì›¹ìº ",
            type="numpy",
            sources=["webcam"],   # ì›¹ìº ë§Œ
            streaming=True        # ìŠ¤íŠ¸ë¦¬ë° ON
        )
        out_cam = gr.Image(label="ë©”ì´í¬ì—… ê²°ê³¼ (ì›¹ìº )")
    with gr.Row():
        txt_res_cam = gr.Textbox(label="ë¶„ì„ ê²°ê³¼ (ì›¹ìº )")
        txt_rec_cam = gr.Textbox(label="ì¶”ì²œ ì œí’ˆ (ì›¹ìº )")

    cam.stream(
        fn=process_oliveyoung_style,
        inputs=cam,
        outputs=[out_cam, txt_res_cam, txt_rec_cam],
        stream_every=0.3   # 0.3ì´ˆë§ˆë‹¤ í”„ë ˆì„ ì²˜ë¦¬ (í•„ìš”í•˜ë©´ ì¡°ì ˆ)
    )

demo.launch(debug=True, share=True)